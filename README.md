  <h1>ğŸ–±ï¸ Gesture Controlled Virtual Mouse</h1>
  <p>An AI-powered, touchless interface that uses hand gestures and voice commands to control the computerâ€”no physical contact required.</p>

  <div class="section">
    <h2>ğŸ” Description</h2>
    <p>This project simplifies human-computer interaction by utilizing state-of-the-art Machine Learning and Computer Vision to detect static and dynamic hand gestures using MediaPipe and CNN. It also includes a voice assistant named <strong>Proton</strong> to handle system operations via speech, all without requiring any special hardware. Compatible with Windows systems.</p>
  </div>

  <div class="section">
    <h2>âœ¨ Features</h2>
    <h3>ğŸ¯ Gesture Recognition:</h3>
    <ul>
      <li>Move Cursor</li>
      <li>Left Click, Right Click, Double Click</li>
      <li>Scroll, Drag & Drop</li>
      <li>Multiple Item Selection</li>
      <li>Volume & Brightness Control</li>
    </ul>
    <h3>ğŸ™ï¸ Voice Assistant â€“ Proton:</h3>
    <ul>
      <li>Start/Stop Gesture Module</li>
      <li>Google Search, Google Maps</li>
      <li>File Navigation, Copy/Paste</li>
      <li>System Time, Sleep/Wake, Exit</li>
    </ul>
  </div>

  <div class="section">
    <h2>âš™ï¸ Getting Started</h2>
    <p><strong>Python Version:</strong> 3.8.5</p>
    <pre><code>git clone https://github.com/yourusername/gesture-virtual-mouse.git
cd gesture-virtual-mouse
pip install -r requirements.txt
python main.py</code></pre>
  </div>

  <div class="section">
    <h2>ğŸ“‚ Code Structure</h2>
    <pre><code>gesture-virtual-mouse/
â”œâ”€â”€ main.py
â”œâ”€â”€ gesture_controller/
â”‚   â”œâ”€â”€ hand_tracker.py
â”‚   â”œâ”€â”€ glove_detector.py
â”‚   â”œâ”€â”€ gesture_classifier.py
â”‚   â””â”€â”€ mouse_controller.py
â”œâ”€â”€ voice_assistant/
â”‚   â”œâ”€â”€ proton.py
â”‚   â”œâ”€â”€ speech_recognizer.py
â”‚   â””â”€â”€ command_executor.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ helpers.py
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ models/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md</code></pre>
  </div>

  <div class="section">
    <h2>ğŸ“œ Flowchart / ğŸ“Œ Architecture</h2>
    <p><strong>Flow:</strong> Start â†’ Hand Detection â†’ Landmark Extraction â†’ Gesture Classifier â†’ Mouse/Voice Action</p>
    <p><strong>Architecture:</strong></p>
    <pre><code>+ Webcam Input
    â†“
+ MediaPipe / Glove Detection
    â†“
+ Landmark / Color Extraction
    â†“
+ Gesture Classifier (CNN / Rules)
    â†“
+ PyAutoGUI (Mouse Actions)
    â†˜
+ Proton (Voice Assistant) â†’ System / Web Commands</code></pre>
   
  <div class="section">
    <h2>ğŸ” Comparative Analysis</h2>
    <table border="1" cellpadding="8">
      <thead>
        <tr>
          <th>Feature</th><th>Traditional Mouse</th><th>Touch Screen</th><th>Gesture Mouse</th>
        </tr>
      </thead>
      <tbody>
        <tr><td>Hardware</td><td>High</td><td>Medium</td><td>Low</td></tr>
        <tr><td>Contactless</td><td>No</td><td>No</td><td>Yes</td></tr>
        <tr><td>Voice Control</td><td>No</td><td>No</td><td>Yes</td></tr>
        <tr><td>Accessibility</td><td>Low</td><td>Medium</td><td>High</td></tr>
        <tr><td>Cost</td><td>Medium</td><td>Low</td><td>High</td></tr>
      </tbody>
    </table>
  </div>

  <div class="section">
    <h2>ğŸ’» Language & Tool Roles</h2>
    <ul>
      <li><strong>Python</strong> - Core development</li>
      <li><strong>MediaPipe</strong> - Hand tracking</li>
      <li><strong>OpenCV</strong> - Video feed & processing</li>
      <li><strong>PyAutoGUI</strong> - Mouse control</li>
      <li><strong>SpeechRecognition</strong> - Voice input</li>
      <li><strong>pyttsx3</strong> - Voice output</li>
    </ul>
  </div>

  <div class="section">
    <h2>ğŸ”§ System Layers</h2>
    <ul>
      <li><strong>Input Layer:</strong> Webcam & Microphone</li>
      <li><strong>Processing Layer:</strong> ML models & command logic</li>
      <li><strong>Output Layer:</strong> Mouse Actions & Voice Response</li>
    </ul>
  </div>

  <div class="section">
    <h2>ğŸ”® Upcoming Features</h2>
    <ul>
      <li>Cross-platform support (Linux, Windows)</li>
      <li>Dynamic gesture recognition (LSTM)</li>
      <li>Depth sensor support (e.g., RealSense)</li>
      <li>Multilingual Voice Assistant (Hindi, German, etc.)</li>
      <li>GUI dashboard for config/logs</li>
    </ul>
  </div>

  <div class="section">
    <h2>ğŸ§  Strategic Recommendations</h2>
    <ul>
      <li>Deploy with PyInstaller or Electron for full desktop use</li>
      <li>Use ONNX or TensorRT for faster inference</li>
      <li>Great for use in accessibility tech, smart homes, or touchless UIs</li>
    </ul>
  </div>

  <footer>
    <p>Â© 2025 Gesture Controlled Virtual Mouse. All rights reserved.</p>
  </footer>
</body>
</html>
